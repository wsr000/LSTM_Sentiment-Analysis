{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score\n",
    "\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "if torch.cuda.is_available():\n",
    "    # 获取当前 GPU 设备的数量\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "\n",
    "    # 遍历每个 GPU，并打印 GPU 的型号\n",
    "    for i in range(num_gpu):\n",
    "        print(f\"GPU {i + 1}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "data_path =  '/content/drive/MyDrive/LSTM_情感分析/data/data.txt'              # 数据集\n",
    "vocab_path = '/content/drive/MyDrive/LSTM_情感分析/data/vocab.pkl'             # 词表\n",
    "save_path = '/content/drive/MyDrive/LSTM_情感分析/saved_dict/lstm.ckpt'        # 模型训练结果\n",
    "embedding_pretrained = \\\n",
    "    torch.tensor(\n",
    "    np.load(\n",
    "        '/content/drive/MyDrive/LSTM_情感分析/data/embedding_Tencent.npz')\n",
    "    [\"embeddings\"].astype('float32'))\n",
    "                                            # 预训练词向量\n",
    "embed = embedding_pretrained.size(1)        # 词向量维度\n",
    "dropout = 0.5                               # 随机丢弃\n",
    "num_classes = 2                             # 类别数\n",
    "num_epochs = 7                             # epoch数\n",
    "batch_size = 128                            # mini-batch大小\n",
    "pad_size = 50                               # 每句话处理成的长度(短填长切)\n",
    "learning_rate = 1e-3                        # 学习率\n",
    "hidden_size = 128                           # lstm隐藏层\n",
    "num_layers = 2                              # lstm层数\n",
    "MAX_VOCAB_SIZE = 10000                      # 词表长度限制\n",
    "UNK, PAD = '<UNK>', '<PAD>'                 # 未知字，padding符号\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    tokenizer = lambda x: [y for y in x]  # 字级别\n",
    "    vocab = pkl.load(open(vocab_path, 'rb'))\n",
    "    # print('tokenizer',tokenizer)\n",
    "    print('vocab',vocab)\n",
    "    print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "    train,dev,test = load_dataset(data_path, pad_size, tokenizer, vocab)\n",
    "    return vocab, train, dev, test\n",
    "\n",
    "def load_dataset(path, pad_size, tokenizer, vocab):\n",
    "    '''\n",
    "    将路径文本文件分词并转为三元组返回\n",
    "    :param path: 文件路径\n",
    "    :param pad_size: 每个序列的大小\n",
    "    :param tokenizer: 转为字级别\n",
    "    :param vocab: 词向量模型\n",
    "    :return: 二元组，含有字ID，标签\n",
    "    '''\n",
    "    contents = []\n",
    "    n=0\n",
    "    with open(path, 'r', encoding='gbk') as f:\n",
    "        # tqdm可以看进度条\n",
    "        for line in tqdm(f):\n",
    "            # 默认删除字符串line中的空格、’\\n’、't’等。\n",
    "            lin = line.strip()\n",
    "            if not lin:\n",
    "                continue\n",
    "            # print(lin)\n",
    "            label,content = lin.split('\t####\t')\n",
    "            # word_line存储每个字的id\n",
    "            words_line = []\n",
    "            # 分割器，分词每个字\n",
    "            token = tokenizer(content)\n",
    "            # print(token)\n",
    "            # 字的长度\n",
    "            seq_len = len(token)\n",
    "            if pad_size:\n",
    "                # 如果字长度小于指定长度，则填充，否则截断\n",
    "                if len(token) < pad_size:\n",
    "                    token.extend([vocab.get(PAD)] * (pad_size - len(token)))\n",
    "                else:\n",
    "                    token = token[:pad_size]\n",
    "                    seq_len = pad_size\n",
    "            # 将每个字映射为ID\n",
    "            # 如果在词表vocab中有word这个单词，那么就取出它的id；\n",
    "            # 如果没有，就去除UNK（未知词）对应的id，其中UNK表示所有的未知词（out of vocab）都对应该id\n",
    "            for word in token:\n",
    "                words_line.append(vocab.get(word, vocab.get(UNK)))\n",
    "            n+=1\n",
    "            contents.append((words_line, int(label)))\n",
    "\n",
    "    train, X_t = train_test_split(contents, test_size=0.4, random_state=42)\n",
    "    dev,test= train_test_split(X_t, test_size=0.5, random_state=42)\n",
    "    return train,dev,test\n",
    "# get_data()\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.x = torch.LongTensor([x[0] for x in data]).to(self.device)\n",
    "        self.y = torch.LongTensor([x[1] for x in data]).to(self.device)\n",
    "    def __getitem__(self,index):\n",
    "        self.text = self.x[index]\n",
    "        self.label = self.y[index]\n",
    "        return self.text, self.label\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "# 以上是数据预处理的部分\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "# 定义LSTM模型\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # 使用预训练的词向量模型，freeze=False 表示允许参数在训练中更新\n",
    "        # 在NLP任务中，当我们搭建网络时，第一层往往是嵌入层，对于嵌入层有两种方式初始化embedding向量，\n",
    "        # 一种是直接随机初始化，另一种是使用预训练好的词向量初始化。\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_pretrained, freeze=False)\n",
    "        # bidirectional=True表示使用的是双向LSTM\n",
    "        self.lstm = nn.LSTM(embed, hidden_size, num_layers,\n",
    "                            bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        # 因为是双向LSTM，所以层数为config.hidden_size * 2\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        # lstm 的input为[batchsize, max_length, embedding_size]，输出表示为 output,(h_n,c_n),\n",
    "        # 保存了每个时间步的输出，如果想要获取最后一个时间步的输出，则可以这么获取：output_last = output[:,-1,:]\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.fc(out[:, -1, :])  # 句子最后时刻的 hidden state\n",
    "        return out\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "# 权重初始化，默认xavier\n",
    "# xavier和kaiming是两种初始化参数的方法\n",
    "def init_network(model, method='xavier', exclude='embedding'):\n",
    "    for name, w in model.named_parameters():\n",
    "        if exclude not in name:\n",
    "            if 'weight' in name:\n",
    "                if method == 'xavier':\n",
    "                    nn.init.xavier_normal_(w)\n",
    "                elif method == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(w)\n",
    "                else:\n",
    "                    nn.init.normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(w, 0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "def plot_acc(train_acc):\n",
    "    sns.set(style='darkgrid')\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    x = list(range(len(train_acc)))\n",
    "    plt.plot(x, train_acc, alpha=0.9, linewidth=2, label='train acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('/content/drive/MyDrive/LSTM_情感分析/results/acc.png', dpi=400)\n",
    "\n",
    "def plot_loss(train_loss):\n",
    "    sns.set(style='darkgrid')\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    x = list(range(len(train_loss)))\n",
    "    plt.plot(x, train_loss, alpha=0.9, linewidth=2, label='train acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('/content/drive/MyDrive/LSTM_情感分析/results/loss.png', dpi=400)\n",
    "\n",
    "# 定义训练的过程\n",
    "def train( model, dataloaders):\n",
    "    '''\n",
    "    训练模型\n",
    "    :param model: 模型\n",
    "    :param dataloaders: 处理后的数据，包含trian,dev,test\n",
    "    '''\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    dev_best_loss = float('inf')\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(\"Start Training...\\n\")\n",
    "    plot_train_acc = []\n",
    "    plot_train_loss = []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        # 1，训练循环----------------------------------------------------------------\n",
    "        # 将数据全部取完\n",
    "        # 记录每一个batch\n",
    "        step = 0\n",
    "        train_lossi=0\n",
    "        train_acci = 0\n",
    "        for inputs, labels in dataloaders['train']:\n",
    "            # 训练模式，可以更新参数\n",
    "            model.train()\n",
    "            # print(inputs.shape)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # 梯度清零，防止累加\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            step += 1\n",
    "            true = labels.data.cpu()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "            train_lossi += loss.item()\n",
    "            train_acci += metrics.accuracy_score(true, predic)\n",
    "            # 2，验证集验证----------------------------------------------------------------\n",
    "        dev_acc, dev_loss = dev_eval(model, dataloaders['dev'], loss_function,Result_test=False)\n",
    "        if dev_loss < dev_best_loss:\n",
    "            dev_best_loss = dev_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        train_acc = train_acci/step\n",
    "        train_loss = train_lossi/step\n",
    "        plot_train_acc.append(train_acc)\n",
    "        plot_train_loss.append(train_loss)\n",
    "        print(\"epoch = {} :  train_loss = {:.3f}, train_acc = {:.2%}, dev_loss = {:.3f}, dev_acc = {:.2%}\".\n",
    "                  format(i+1, train_loss, train_acc, dev_loss, dev_acc))\n",
    "    plot_acc(plot_train_acc)\n",
    "    plot_loss(plot_train_loss)\n",
    "    # 3，验证循环----------------------------------------------------------------\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    test_acc, test_loss = dev_eval(model, dataloaders['test'], loss_function,Result_test=True)\n",
    "    print('================'*8)\n",
    "    print('test_loss: {:.3f}      test_acc: {:.2%}'.format(test_loss, test_acc))\n",
    "\n",
    "def result_test(real, pred):\n",
    "    cv_conf = confusion_matrix(real, pred)\n",
    "    acc = accuracy_score(real, pred)\n",
    "    precision = precision_score(real, pred, average='micro')\n",
    "    recall = recall_score(real, pred, average='micro')\n",
    "    f1 = f1_score(real, pred, average='micro')\n",
    "    patten = 'test:  acc: %.4f   precision: %.4f   recall: %.4f   f1: %.4f'\n",
    "    print(patten % (acc, precision, recall, f1,))\n",
    "    labels11 = ['negative', 'active']\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cv_conf, display_labels=labels11)\n",
    "    disp.plot(cmap=\"Blues\", values_format='')\n",
    "    plt.savefig(\"/content/drive/MyDrive/LSTM_情感分析/results/reConfusionMatrix.tif\", dpi=400)\n",
    "\n",
    "# 模型评估\n",
    "def dev_eval(model, data, loss_function,Result_test=False):\n",
    "    '''\n",
    "    :param model: 模型\n",
    "    :param data: 验证集集或者测试集的数据\n",
    "    :param loss_function: 损失函数\n",
    "    :return: 损失和准确率\n",
    "    '''\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data:\n",
    "            outputs = model(texts)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss_total += loss.item()\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    if Result_test:\n",
    "        result_test(labels_all, predict_all)\n",
    "    else:\n",
    "        pass\n",
    "    return acc, loss_total / len(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 设置随机数种子，保证每次运行结果一致，不至于不能复现模型\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    torch.cuda.manual_seed_all(1)\n",
    "    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Loading data...\")\n",
    "    vocab, train_data, dev_data, test_data = get_data()\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(TextDataset(train_data), batch_size, shuffle=True),\n",
    "        'dev': DataLoader(TextDataset(dev_data), batch_size, shuffle=True),\n",
    "        'test': DataLoader(TextDataset(test_data), batch_size, shuffle=True)\n",
    "    }\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = Model().to(device)\n",
    "    init_network(model)\n",
    "    train(model, dataloaders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch for Deeplearning",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
